---
title: "conclusionSection"
author: "Trevor Isaacson"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE)
library(tidyverse)
library(rstanarm)
library(GGally)
library(ggthemes)
library(scales)
library(bayesplot)
library(loo)
library(ggplot2)
```

```{r}
set.seed(551)
```

```{r echo = FALSE}
# read data from .csv file
data_raw <- read.csv('ToyotaCorollaData.csv')

data <- data_raw %>%
  # factor categorical predictors
  mutate_at(c('Metallic', 'Automatic', 'Doors', 'Cylinders', 'Gears', 
              'Guarantee', 'BOVAG', 'Fuel_Type', 'Mfg_Month', 'Mfg_Year'),
            as.factor) %>%
  # remove singular predictor
  select(-Cylinders)

head(data)
```

Scale data and log transform `Price`.
```{r echo = FALSE}
# log transform Price and scale predictors
data_scale <- cbind(log(data$Price),
                    (data %>% 
                       select(-Price) %>%
                       mutate_if(is.numeric, scale))) %>%
  as.data.frame()
names(data_scale) <- names(data)
```

```{r, warning=FALSE, message=FALSE}
fit4 <- stan_glm(Price ~ Age + Mfg_Month + Mfg_Year + KM + Fuel_Type + HP + 
                   Metallic + QuartTax + Weight + Guarantee + BOVAG + Period,
                 data = data_scale,
                 refresh = 0,
                 iter = 5000)
```


# Conclusion
## Final Model and Fitted Equation
For our final model, we decided to use model $4$.  This model is constructed using horse-shoe selected predictors with the scaled data and default priors.  The horseshoe selected predictors include `Age`, `Mfg_Month`, `Mfg_Year`, `KM`, `Fuel_Type`, `HP`, `Metallic`, `QuartTax`, `Weight`, `Guarantee`, `BOVAG`, and `Period`.  Also, using the scaled numeric predictors and a log-transformation of car price to restrict prediction prices to positive values only, this model is able to better predict selling price compared to other models.


## Tables of Estimated Coefficients/Standard Errors
```{r}
print(fit4, digits = 3)
```


## Interpretation and Discussion
The intercept coefficient can be interpreted as the expected log(price) of a non-metallic, CNG-fueled car that was manufactured in January of 1999 with no manufacturer or BOVAG guarantee and a value of $0$ for all numeric predictors included in the model.  The coefficients estimates for the numeric predictors can be interpreted as the expected percent difference in predicted car price associated with a one standard deviation increase in the value of the predictor, with all other predictors remaining constant.  Overall, this model had an in-sample Bayesian $R^2$ value of 0.882 and a leave one out adjusted $R^2$ value of 0.87.  This confirms our model isn't overfitting while also showing a relatively high $R^2$ value.  

## Predictive Plots
The final fitted model should look like our data.  By drawing from the predictive distribution and comparing it to the distribution of the response variable, we can determine if our model is fitting appropriately.  In the plot below, we see our predictive distribution tracks the response distribution well increasing our confidence in the model.  
```{r}
fit4_rep = posterior_predict(fit4)
ppc_dens_overlay(data_scale$Price, fit4_rep) + scale_y_continuous(breaks=NULL)
```

### Check your assumptions
Checking the assumptions of our final model, we find the model includes all the relevant predictors as we chose these using the horseshoe prior and forcing predictors to be highly related with the response.  The outcome measure accurately reflects our prediction interest and is generalized to all Toyota Corollas.  Looking at the residual plots, we see there are no patterns and trends within the residuals vs fitted values plot.  Most values are within $2$ standard residuals of $0$ and the values are spread across the $0$ line.  There might be some clumping but nothing big enough to question the model.  There aren't any heavy tails in the qq-norm plot and the values closely align with the red line.    

```{r fig.show="hold", out.width="50%", echo = FALSE}
plt_res_fit =ggplot(mapping = aes(x = fit4$fitted.values, y = fit4$residuals)) +
    geom_point(alpha = .5) +
    geom_hline(yintercept = 0, col = 'red', size = .2) +
    geom_hline(yintercept = 2*sigma(fit4), size = 0.1) +
    geom_hline(yintercept = -2*sigma(fit4), size = 0.1) +
    labs(title = 'Residuals vs Fitted Values Plot',
         x = 'Fitted Values',
         y = 'Residuals')

n <- length(fit4$residuals)
quants <- qnorm((1:n / n))
plt_QQ <- ggplot(mapping = aes(x = quants,  y = sort(scale(fit4$residuals)))) +
  geom_point(alpha = .5) +
  geom_abline(intercept = 0,
              slope = 1,
              col = 'red',
              size = .2) +
  labs(title = 'QQ-Norm Plot',
       x = 'Theoretical Quantiles',
       y = 'Sorted Residuals')

plt_res_fit
plt_QQ
```


## Others results (as appropriate)
Because the goal of this study is prediction, our final model has great observed predictive powers using leave one out cross validation and also k-fold cross validation.  Using 


```{r}
# Put smaller table cv results here
```



## Refer back to the purpose of the study
In all, the purpose of this study was to predict the selling price of used Toyota Corollas and ensure a small profit based on their new purchase and trade-in promotion.  Based on several variables, we were able to fit this model to help the dealership closely estimate the final selling price for their used cars.  With this model, the dealership can now ensure a reasonable profit by plugging in the characteristics of each individual car into this model and output a predicted selling price.  This will result in more accurate selling prices and higher profits for the dealer.       

