---
title: "Case Study Trevor"
author: "Trevor Isaacson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rstanarm)
library("rstantools")
library(GGally)
library(scales)
library(ggthemes)
library("loo")
library("ggplot2")
library("bayesplot")
theme_set(bayesplot::theme_default(base_family = "sans"))
set.seed(551)
SEED = 551
```

# Data Exploration

### Import Data
```{r}
data_raw <- read.csv('ToyotaCorollaData.csv')

data <- data_raw %>%
   mutate_at(c('Metallic', 'Automatic', 'Doors', 'Cylinders', 'Gears', 
              'Guarantee', 'BOVAG', "Fuel_Type"),
            as.factor) %>%
  # cyclinders has one factor, remove mfg_month and mfg_year as basically same as age
  select(-c(Cylinders, Mfg_Month, Mfg_Year)) %>%
  drop_na()
head(data)

cont_predictors = c("Age", "KM", "HP", "CC", 
                    "QuartTax", "Weight", "Period")

str(data)
```
## Check Log Price
```{r}
ggplot(data = data, mapping = aes(x = log(Price))) +
  geom_histogram()
```


## Default Linear Model    
This figure shows the predictors without standardization 
```{r}
fit0 <- stan_glm(Price ~ ., data = data, refresh=0)
```

```{r}
p0 <- mcmc_areas(fit0, pars=vars(-'(Intercept)',-sigma),
                 prob_outer=0.95, area_method = "scaled height")
p0
```

## Standardize continuous predictors 
Makes for easy comparison of relevances
```{r}
dataStd = data
dataStd[, cont_predictors] = scale(data[ , cont_predictors])
dataStd
```

## Default Weak Prior on Coefficients
Fit a regression model with default weak priors
```{r}
fit1 = stan_glm(Price ~ ., data = dataStd, refresh = 0, seed = SEED)
```

Plot posterior marginals of coefficients
```{r}
p1 <- mcmc_areas(fit1, pars=vars(-'(Intercept)', -sigma),
                 prob_outer=0.95, area_method = "scaled height")
p1
```
This figure shows that after all predictors have been standardized to have equal standard deviation, the uncertainties on the relevances are similar.  We can see a many predictors have high relevance based on their distance from 0.  

## Horseshoe prior
```{r}
p0 = 6
p = ncol(dataStd)
n = nrow(dataStd)

slab_scale = sd(dataStd$Price)/sqrt(p0)*sqrt(0.3)
global_scale = (p0 / (p - p0))/sqrt(n)
fit2 = stan_glm(Price ~ ., data = dataStd, refresh = 0,
                prior = hs(global_scale = global_scale, 
                           slab_scale = slab_scale))
```

Plot posterior marginals of coefficients
```{r}
p2 <- mcmc_areas(fit2, pars=vars(-'(Intercept)', -sigma),
                 prob_outer=0.95, area_method = "scaled height")
p2
```

This figure shows that the regularized horseshoe prior shrinks the posterior for many regression coefficients towards 0, showing the more relevant predictors.  

## Compare R^2
```{r}
round(median(bayes_R2(fit0)), 3)
round(median(loo_R2(fit0)), 3)

round(median(bayes_R2(fit1)), 3)
round(median(loo_R2(fit1)), 3)

round(median(bayes_R2(fit2)), 3)
round(median(loo_R2(fit2)), 3)
```

## Bayesian R^2 Distribution
```{r}
ggplot() + geom_histogram(aes(x=bayes_R2(fit2), breaks = 25)) +
  xlim(c(0.80,0.925)) +
  scale_y_continuous() +
  labs(x="Bayesian R^2", y="")
```

## Compare Models
```{r}
loo0 = loo(fit0, k_threshold = 0.7)
loo1 = loo(fit1, k_threshold = 0.7)
loo2 = loo(fit2, k_threshold = 0.7)
loo_compare(loo0 ,loo1, loo2)
```

# Subset of Covariates

## Default Priors
```{r}
fit_subset1_default = stan_glm(Price ~ Age + KM + Fuel_Type + HP + QuartTax + Weight 
                       + Guarantee + BOVAG + Period, 
                       data = dataStd, 
                       refresh = 0)
```

```{r warning = FALSE}
print(paste("In Sample Bayesian R^2: ",round(median(bayes_R2(fit_subset1_default)), 3)))
print(paste("CV LOO R^2: ", round(median(loo_R2(fit_subset1_default)), 3)))

loo_subset1 = loo(fit_subset1_default)
```

```{r}
ps1 <- mcmc_areas(fit_subset1_default, pars=vars(-'(Intercept)', -sigma),
                 prob_outer=0.95, area_method = "scaled height")
ps1

res = resid(fit_subset1_default)
plot(x = fitted(fit_subset1_default), y = res, main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Residuals")
abline(0, 0)
```


## Interaction w/ HP + Weight
More HP, bigger engine/more weight?
```{r}
fit_subset2_default = stan_glm(Price ~ Age + KM + HP + Weight + 
                          QuartTax + Fuel_Type + Guarantee + BOVAG + Period +
                          HP:Weight, 
                       data = dataStd, 
                       refresh = 0,
                       )
```

```{r}
print(fit_subset2_default, 4)

res = resid(fit_subset2_default)
plot(x = fitted(fit_subset2_default), y = res, main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Residuals")
abline(0, 0)
```

## Interaction w/ BOVAG + Guarantee

```{r}
fit_subset3_default = stan_glm(Price ~ Age + KM + HP + Weight + 
                          QuartTax + Fuel_Type + Guarantee + BOVAG + Period +
                          BOVAG:Guarantee, 
                       data = dataStd, 
                       refresh = 0,
                       )
```

```{r}
print(fit_subset3_default, 4)

res = resid(fit_subset3_default)
plot(x = fitted(fit_subset3_default), y = res, main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Residuals")
abline(0, 0)
```

## Interaction w/ Age + KM

```{r}
fit_subset4_default = stan_glm(Price ~ Age + KM + HP + Weight + 
                          QuartTax + Fuel_Type + Guarantee + BOVAG + Period +
                          Age:KM, 
                       data = dataStd, 
                       refresh = 0,
                       )
```

```{r}
print(fit_subset4_default, 4)

res = resid(fit_subset4_default)
plot(x = fitted(fit_subset4_default), y = res, main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Residuals")
abline(0, 0)
```


## Interaction w/ Age + KM and HP + Weight

```{r}
fit_subset5_default = stan_glm(Price ~ Age + KM + HP + Weight + 
                          QuartTax + Fuel_Type + Guarantee + BOVAG + Period +
                          Age:KM + HP:Weight, 
                       data = dataStd, 
                       refresh = 0,
                       )
```

```{r}
print(fit_subset5_default, 4)

res = resid(fit_subset5_default)
plot(x = fitted(fit_subset5_default), y = res, main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Residuals")
abline(0, 0)
```


```{r warning = FALSE}
loo_1 = loo(fit_subset1_default)
loo_2 = loo(fit_subset2_default)
loo_3 = loo(fit_subset3_default)
loo_4 = loo(fit_subset4_default)
loo_5 = loo(fit_subset5_default)
loo_compare(loo_1, loo_2, loo_3, loo_4, loo_5)
```

```{r warning = FALSE}
r2values = data.frame(fit = c(1,2,3,4,5), 
                      bayesian = c(round(median(bayes_R2(fit_subset1_default)), 3),
                                  round(median(bayes_R2(fit_subset2_default)), 3),
                                  round(median(bayes_R2(fit_subset3_default)), 3),
                                  round(median(bayes_R2(fit_subset4_default)), 3),
                                  round(median(bayes_R2(fit_subset5_default)), 3)), 
                      LOO_CV = c(round(median(loo_R2(fit_subset1_default)), 3),
                                 round(median(loo_R2(fit_subset2_default)), 3),
                                 round(median(loo_R2(fit_subset3_default)), 3),
                                 round(median(loo_R2(fit_subset4_default)), 3),
                                 round(median(loo_R2(fit_subset5_default)), 3)))

r2values
```


```{r}
folds = kfold_split_random(K = 10, N = n)
kcv_1 = kfold(fit_subset1_default, folds = folds)
kcv_2 = kfold(fit_subset2_default, folds = folds)
kcv_3 = kfold(fit_subset3_default, folds = folds)
kcv_4 = kfold(fit_subset4_default, folds = folds)
kcv_5 = kfold(fit_subset5_default, folds = folds)
loo_compare(kcv_1, kcv_2, kcv_3, kcv_4, kcv_5)

```





